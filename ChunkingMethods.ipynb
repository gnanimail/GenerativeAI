{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fb6cce-7374-4922-87e9-9c9fe90d140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eaa031c-2966-49ee-a85f-2689506bc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks typically built with a transformer-based architecture. Some recent implementations are based on alternative architectures such as recurrent neural network variants and Mamba (a state space model).\n",
    "\n",
    "LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.\n",
    "\n",
    "Some notable LLMs are OpenAI GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google's PaLM and Gemini (used in Bard), Microsoft's Copilot, Meta LLaMA family of open-source models, and Anthropic's Claude models.\n",
    "\n",
    "Although sometimes matching human performance, it is not clear they are plausible cognitive models. At least for recurrent neural networks it has been shown that they sometimes learn patterns which humans do not learn, but fail to learn patterns that humans typically do learn.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26bdc1d1-2184-4758-b16a-190f005733be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks typically built with a transformer-based architecture. Some recent implementations are based on alternative architectures such as recurrent neural network variants and Mamba (a state space model).\\n\\nLLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.\\n\\nSome notable LLMs are OpenAI GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google\\'s PaLM and Gemini (used in Bard), Microsoft\\'s Copilot, Meta LLaMA family of open-source models, and Anthropic\\'s Claude models.\\n\\nAlthough sometimes matching human performance, it is not clear they are plausible cognitive models. At least for recurrent neural networks it has been shown that they sometimes learn patterns which humans do not learn, but fail to learn patterns that humans typically do learn.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5146e68-26d0-453f-a35b-b98d7c95028e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1541"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93459652-6e05-47d9-8463-d84db95d9019",
   "metadata": {},
   "source": [
    "### Split by Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccc2e06-84fb-4408-954e-7a6e5f40d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1a6f4d-4d78-4ee2-9fee-896f0a56d76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 529, which is longer than the specified 400\n",
      "Created a chunk of size 511, which is longer than the specified 400\n"
     ]
    }
   ],
   "source": [
    "text_chunks = CharacterTextSplitter(chunk_size=400,\n",
    "                                    chunk_overlap=10,\n",
    "                                    separator=\"\\n\",\n",
    "                                    length_function=len,\n",
    "                                    is_separator_regex=False)\n",
    "\n",
    "chunks = text_chunks.split_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320ff2c6-5a66-46a3-a015-31f67eccd086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks in the Paragraph: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of chunks in the Paragraph:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d025a85-b596-4970-9fd4-1c75b184eac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0: A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks typically built with a transformer-based architecture. Some recent implementations are based on alternative architectures such as recurrent neural network variants and Mamba (a state space model).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chunk #1: LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chunk #2: Some notable LLMs are OpenAI GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google's PaLM and Gemini (used in Bard), Microsoft's Copilot, Meta LLaMA family of open-source models, and Anthropic's Claude models.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chunk #3: Although sometimes matching human performance, it is not clear they are plausible cognitive models. At least for recurrent neural networks it has been shown that they sometimes learn patterns which humans do not learn, but fail to learn patterns that humans typically do learn.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,_ in enumerate(chunks):\n",
    "    print(f\"Chunk #{i}: {chunks[i]}\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b38413-8922-428e-ad32-38791a71c193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0, Size:529\n",
      "Chunk #1, Size:511\n",
      "Chunk #2, Size:217\n",
      "Chunk #3, Size:277\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(chunks):\n",
    "    print(f\"Chunk #{i}, Size:{len(chunks[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a4342-b60e-4b42-ab63-80ac8b1dd5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9b518-635f-4081-80aa-db6e450e2fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9289973d-e02d-479e-9744-b9b155dfac9a",
   "metadata": {},
   "source": [
    "### Recursive Split by Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c3fa661-a583-4b70-93a8-fec8d3abf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7775f146-dca6-4803-be2d-f994a8661ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = RecursiveCharacterTextSplitter(chunk_size=400,\n",
    "                                             chunk_overlap=10,\n",
    "                                             length_function=len,\n",
    "                                             is_separator_regex=False)\n",
    "\n",
    "chunks = text_chunks.split_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57fc694b-b500-4672-b90f-0f436714c44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks in the Paragraph: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of chunks in the Paragraph:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54fa4455-4018-46aa-9a84-05662cd69011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0: A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks typically built with a transformer-based architecture. Some recent\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chunk #1: recent implementations are based on alternative architectures such as recurrent neural network variants and Mamba (a state space model).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chunk #2: LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chunk #3: syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chunk #4: Some notable LLMs are OpenAI GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google's PaLM and Gemini (used in Bard), Microsoft's Copilot, Meta LLaMA family of open-source models, and Anthropic's Claude models.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chunk #5: Although sometimes matching human performance, it is not clear they are plausible cognitive models. At least for recurrent neural networks it has been shown that they sometimes learn patterns which humans do not learn, but fail to learn patterns that humans typically do learn.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,_ in enumerate(chunks):\n",
    "    print(f\"Chunk #{i}: {chunks[i]}\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f500d5-d100-4bef-8c64-2bb3a56883ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85caa01-ae17-4259-b4e8-7a2840192489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e060f312-5cd4-4ebd-84a3-28388cc5233e",
   "metadata": {},
   "source": [
    "### Tiktoken Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdb4d6-53e8-437e-864f-c08e7eac90da",
   "metadata": {},
   "source": [
    "Tiktoken tokenizer has been created by OpenAI for their family of models. Using this strategy, the split still happens based on the character. \n",
    "However, the length of the chunk is determined by the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5901bedc-66c2-4a26-846b-9f5602cbb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "992668b6-909d-4689-b5fe-029df6c02345",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=400,\n",
    "                                  chunk_overlap=10,\n",
    "                                  length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c07ca0ea-a72b-4a27-a725-0d540204f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f7477aa-70a2-4fea-9704-9aa70d6b5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f604af4a-051d-4e04-aebb-a5c6b7ee5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks Created: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of chunks Created:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5966d0e3-bf0a-4aab-8f5c-d75607354c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Tokens in the document: 314 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Number of Tokens in the document: {len(encoding.encode(content))} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb99c86-bc24-4d7d-b215-33ad36667b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35e5be-6001-4077-84e6-45a098578acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd3a269-5e98-4390-a619-b0f2ac3843f3",
   "metadata": {},
   "source": [
    "### Hugging Face Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d21878-d15b-49c6-8ee4-d5206d54caf0",
   "metadata": {},
   "source": [
    "Hugging Face has become the go-to platform for anyone building apps using LLMs or even other models. All models available via Hugging Face are also accompanied\n",
    "by their tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61343a15-8ca0-4dca-961a-0f5836aefb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6de78d30-f385-4fcf-9152-e4c894c800f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a03bc7c7-b723-4a3f-93cf-e87c48992033",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer,\n",
    "                                                                          chunk_size=400,\n",
    "                                                                          chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77d04f40-626c-467d-8775-a035291ffea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80499c16-6e54-4b46-a6bc-75652406349e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks typically built with a transformer-based architecture. Some recent implementations are based on alternative architectures such as recurrent neural network variants and Mamba (a state space model).\\n\\nLLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.\\n\\nSome notable LLMs are OpenAI GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google\\'s PaLM and Gemini (used in Bard), Microsoft\\'s Copilot, Meta LLaMA family of open-source models, and Anthropic\\'s Claude models.\\n\\nAlthough sometimes matching human performance, it is not clear they are plausible cognitive models. At least for recurrent neural networks it has been shown that they sometimes learn patterns which humans do not learn, but fail to learn patterns that humans typically do learn.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cb7b3-2ba1-48c3-9000-64bbe6936df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d77e71-1f87-45f2-83b2-0f9bedcb6e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679f816-98ab-4463-a534-308e6e091c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
